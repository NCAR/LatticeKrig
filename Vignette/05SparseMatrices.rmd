# Using Sparse Matrices

LatticeKrig, along with other statistical models for large spatial data sets, make use of sparse linear algebra for efficient computation. Sparse matrices are generated in the LK model in two ways. The basis functions have compact support, meaning they are 0 outside a fixed region, so many of the entries in the basis function matrix will be 0. The precision matrix for the coefficients is also constructed to be sparse.  

Computing with sparse matrices can be much faster than the equivalent dense matrices, since one can save memory by only keeping track of the indices and values of the nonzero entries, and algorithms can skip all of the 0 entries. This optimization makes sparse matrix computations on large data sets orders of magnitude faster than the traditional corresponding computations.

In this package, we use the \ttt{spam} package for sparse matrices. This package has built-in methods for storing, multiplying, and solving sparse matrices, as well as finding their Cholesky decomposition, all of which are used heavily in LatticeKrig. The Cholesky decomposition of a matrix $A$ finds the lower triangular matrix $L$ such that $L L^T = A$. This is heavily used in LatticeKrig because it is significantly easier to solve a triangular system than a normal system ($\mathcal O(n^2)$ v.s. $\mathcal O(n^3)$), which combines with the optimization of using sparse matrices to make our calculations practical on very large data sets. 

## Timing sparse v.s. dense matrices

To demonstrate the difference sparse matrices can make, we will time how long it takes to compute the Cholesky decomposition with and without taking advantage of the sparsity. We will consider $100 \times 100$, $300 \times 300$, $1000 \times 1000$, and $3000 \times 3000$ matrices. For each size, we will first do the Cholesky decomposition on the full matrix representation, then on the sparse representation. Recall that even though many of the matrix entries are 0, the decomposition doesn't take advantage of this feature unless we use the sparse formatting.
```{r SparseCholComparison}
for(N in c(100, 300, 1000, 3000)) {
  SMat <- LKDiag(c(-1, 5, -1), N)
  FMat <- spam2full(SMat)
  cat("Matrix size: ", N, "\n")
  cat("Full Matrix:\t\t")
  startTime <- Sys.time()
  FChol <- chol(FMat)
  stopTime <- Sys.time()
  delta <- stopTime - startTime
  print(delta)
  
  cat("Sparse Matrix:\t")
  startTime <- Sys.time()
  SChol <- chol(SMat)
  stopTime <- Sys.time()
  delta <- stopTime - startTime
  print(delta)
  cat("\n")
}
```

As you can see from the output above, with sizable inputs the sparse matrix computation is thousands of times faster than the traditional dense matrix computation, and this advantage grows even faster with larger inputs.