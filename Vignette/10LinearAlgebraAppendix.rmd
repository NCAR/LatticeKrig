\newcommand{\bld}{\mathbf}

# Appendix A: The Linear Algebra of Kriging

Suppose we have a vector$\bld{y}$ of observations, where each observation $y_i$ is taken at location $\bld s_i$, and a covariate matrix $X$. Assuming that the observations are a linear combination of the covariates with a Gaussian process of mean 0, we have
\[ \bld y = X \bld d + \epsilon\]
where $\epsilon \sim MN(\bld 0, \Sigma)$ for some covariance matrix $\Sigma$. We can then make assumptions to determine the form of $\Sigma$: Assuming the process is stationary, $\sigma_{ij}$ will only depend on the vector $\bld s_i - \bld s_j$; assuming the process is isotropic, $\sigma_{ij}$ will only depend on the scalar $||\bld s_i - \bld s_j||$, which also means that $\Sigma$ will be symmetric. This then allows us to establish a covariance function, $c$, such that $\sigma_{ij} = c(||\bld s_i - \bld s_j||)$. The covariance function describes how strongly correlated observations at varying distances are; as such, we would expect that $c(0) = 1$ and $\lim_{x \to \infty} c(x) = 0$. We can make further assumptions about the covariance function to make computations easier. In LatticeKrig, we assume the covariance function is a Wendland function, which has compact support on $[0, 1]$. This compact support will lead to a sparse $\Sigma$, which makes computing with $\Sigma$ significantly faster and allows us to compute kriging estimates on very large data sets in a reasonable amount of time. Alternatively, in fixed-rank kriging, it is assumed that $\Sigma = S^T K S$, where each column of $S$ is a set of basis functions evaluated at a location of an observation. This form of $\Sigma$ also makes some computations easier, making it another technique for kriging on large data sets.


In LatticeKrig, we assume that $\epsilon = \Phi \bld{c} + \bld{e}$, where $\Phi$ is a matrix of radial basis functions (so $\phi_{ij}$ is the $j^{th}$ basis function evaluated at the $i^{th}$ point) and each basis function is the same except for a shift in location, $\bld c$ is the vector of coefficents that each basis function is weighted by, and $\bld e$ is the vector of measurement errors, distributed $N(0, \sigma^2 I)$. Thus, our total model is $\bld y = X \bld d + \Phi \bld c + \bld e$. We can't predict measurement error, so instead we focus on predicting $X \bld d + \Phi \bld c$ at new locations. The matrix of covariates $X$ and the matrix of basis functions $\Phi$ are both determined from the points we choose to predict at: the unknowns we need to estimate are $\bld c$ and $\bld d$. We estimate $\bld d$ by using the generalized least squares estimate: $\bld d = (X^T \Sigma^{-1} X)^{-1} X^T \Sigma^{-1}$. Estimating $\bld c$ is more involved. First, we partition $X$ and $\bld y$ into two parts: the parts corresponding to the known data, $X_1$ and $\bld y_1$, and the parts corresponding to the data we want to predict, $X_2$ and $\bld y_2$. Since we assume that $y$ follows a Gaussian process, we can write
\[
\begin{pmatrix} \bld y_1 \\ \bld y_2 \end{pmatrix} \sim N\left( \begin{pmatrix} \mu_1 \\ \mu_2 \end{pmatrix}, \begin{bmatrix} \Sigma_{11} & \Sigma_{12} \\ \Sigma_{21} & \Sigma_{22} \end{bmatrix} \right).
\]
It is known from multivariate probability theory that
\[
E[\bld y_2 | \bld y_1] = \mu_2 + \Sigma_{21} \Sigma_{11}^{-1} (\bld y_1 - \mu_1).
\]
Where $\mu_1$ and $\mu_2$ are the means of $\bld y_1$ and $\bld y_2$, respectively. Since $\epsilon = \Phi \bld c + \bld e$ has mean 0, the mean must come from the $X \bld d$ term: that is, $\mu_1 = X_1 \bld d$ and $\mu_2 = X_2 \bld d$. Since $E[\bld y_2 | \bld y_1]$ is the best estimator of the values of $\bld y_2$, we want to find a value of $\bld c$ that makes our model reproduce this estimator, so we set $E[\bld y_2 | \bld y_1] = X_2 \bld d + \Phi_2 \bld c$, where $\Phi_2$ is the matrix of all basis functions evaluated at the points where we're trying to predict y. This gives us the equation
\[
X_2 \bld d + \Phi_2 \bld c = X_2 \bld d + \Sigma_{21} \Sigma_{11}^{-1} (\bld y_1 - \mu_1).
\]
Now, consider what happens if we make the covariance function and basis function match. Each entry in $\Sigma_{21}$ is the covariance function of the distance between the $j^{th}$ data point and the $i^{th}$ prediction point, which would be equal to the basis function of the distance between the $j^{th}$ data point and the $i^{th}$ prediction point, which is each entry in $\Phi_2$. This means we can substitute $\Phi_2 = \Sigma_{21}$ into our equation, giving us:
\begin{align*}
X_2 \bld d + \Phi_2 \bld c &= X_2 \bld d + \Sigma_{21} \Sigma_{11}^{-1} (\bld y_1 - \mu_1) \\
\Phi_2 \bld c &= \Sigma_{21} \Sigma_{11}^{-1} (\bld y_1 - \mu_1) \\
\Phi_2 \bld c &= \Phi_2 \Sigma_{11}^{-1} (\bld y_1 - \mu_1) \\
\bld c &= \Sigma_{11}^{-1} (\bld y_1 - \mu_1)
\end{align*}
and so we arrive at a formula for $\bld c$ which, when the basis function equals the covariance function, gives us the best estimator for $\bld y_2$.