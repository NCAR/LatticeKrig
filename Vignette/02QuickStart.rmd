# Quick Start Guide

In this section, we will lay out the bare essentials of the package as a quick overview for the impatient reader. To fit a surface and interpolate data using \ttt{LatticeKrig}, the only required arguments are, naturally, the measurement locations (formatted in a matrix where each row indexes one location) and measurement values. However, we highly recommend using some of the optional parameters to customize the model to your specific data problem - several ways to do this are illustrated in this vignette. Calling the \ttt{LatticeKrig} function and passing in the locations and values will produce an \ttt{LatticeKrig} object that contains all the information needed to predict the variable at any location. Also, some spatial parameters are estimated by maximum likelihood if not specified. 

For a simple, 1-dimensional example, we will take our locations to be 50 randomly spaced points on the interval $[-6, 6]$, and our observations to be the values of $\sin(x)$ at these locations with some added error. The goal of our kriging fit is to estimate this smooth curve from the observations.

## One dimensional LatticeKrig example

```{r QuickStart1D}
#Making the synthetic data
set.seed(223)
locations <- runif(50, min=-6, max=6)
locations <- as.matrix(locations)
observations <- sin(locations) + rnorm(50, sd = 1e-1)
#Fit to the data, with parameters sigma and rho found by maximum likelihood.
kFit1D <- LatticeKrig(locations, observations)
```

Now we will print out the \ttt{LKrig} object: this list features the data's estimated covariance scale \ttt{rho} and estimated standard measurement error \ttt{sigma}, and the basis function description: the type of basis function, how distance is measured, and the number and spacing of basis functions. In this example, all of this information is determined by \ttt{LatticeKrig} from defaults, but can be changed with optional parameters.

```{r QuickStart1DPrintout}
print(kFit1D)
```

In the printout above, \ttt{sigma} is the estimated standard deviation of the measurement error; we set it to be 0.1, so the estimate 0.106 is great. EDF is a measurement of how strictly the model matches the original data; when EDF = 1, the model will be a straight line; when EDF equals the number of observations, the model will exactly match each recorded data point. We can also see the type of basis function used: in this case it is the Wendland function. We also have the default 3 levels to capture different effect scales.

##Plotting the results

Now, we'll make a plot of the original 50 data points and the true function ($\sin(x)$) and the \ttt{LatticeKrig} fit at 200 equally spaced points to compare them.
```{r QuickStart1DPlot}
xGrid <- seq(-2*pi, 2*pi, len=200)
prediction <- predict(kFit1D, xGrid)
plot(locations, observations, main="1-Dimensional LatticeKrig Example", 
     xlab="Location", ylab="Measured Value")
lines(xGrid, sin(xGrid), col='blue')
lines(xGrid, prediction, col='red', lty=2, lwd=2)
```

For this example, the fitted curve (in red) matches the true function (in blue) rather closely, though error increases near the endpoints and we underestimate some peaks and troughs.

## Inference and error analysis

Although it is beyond the scope of this Vignette to go into the details of conditional simulation it is useful to explain how the package is designed to do this computation -- and what it is good for! Suppose you have fit a model to data, with the results in \ttt{MyFit} as an \ttt{LKrig} or \ttt{LatticeKrig} object and suppose \ttt{Z1} are the covariates at the locations \ttt{x1}.  
The following code generates 10 draws from the distribution of the unknown process *given* (i.e. conditional on) the observations. This random sample is often called an ensemble. As a frequentist-based package, the conditioning in LatticeKrig also assumes that sigma, rho, alpha and a.wght covariance parameters are known. (A Bayesian approach would also sample these from their posterior distribution.)

```{r eval=FALSE}
 aDraw<- LKrig.sim.conditional( MyFit, M=10, x.grid= x1, Z.grid=Z1)
```
The interpretation of \ttt{aDraw} is that each column of aDraw is an equally likely representation of the process and linear model at locations \ttt{x1} given the observed data. As M becomes large the sample mean of the ensemble will converge to the estimate from LKrig. These simulations are easier to compute than the standard error for large data sets and so they are used to estimate the standard error of a model in LatticeKrig. The sample covariances of the ensemble will converge to the correct covariance matrix expressing the unceratinty in the estimate.

This first plot shows the 95\% confidence intervals for the individual locations, based on a collection of simulations. 

```{r QuickStart1DErrorPlot1}
#simulates 50 curves based on the given data
uncertainty <- LKrig.sim.conditional(kFit1D, x.grid = as.matrix(xGrid), M=50)
#making a 95% confidence interval for each data point
sdVec <- apply(uncertainty$g.draw, 1, sd)
gamma <- qnorm(0.975)
upperBound <- prediction + gamma*sdVec
lowerBound <- prediction - gamma*sdVec
plot(xGrid, upperBound, type="l", ylim = c(min(lowerBound), max(upperBound)),
     main = "Data with Confidence Intervals", xlab = "Location", ylab = "Measured Value")
envelopePlot(xGrid, y1 = lowerBound, y2 = upperBound)
points(locations, observations)
lines(xGrid, prediction, col='red', lty=2, lwd=2)
```

This next plot adds a collection of simulations based on the fitted curve and the 95\% confidence envelope for the entire fitted curve.

```{r QuickStart1DErrorPlot2}
#computing the number of standard deviations needed to make the envelope contain 95% of the data
norm <- (uncertainty$g.draw - as.vector(prediction)) / sdVec
gammas <- apply(abs(norm), 2, max)
gamma <- quantile(gammas, 0.95)
#making the upper and lower bounds and plotting the envelope
upperBound <- prediction + gamma*sdVec
lowerBound <- prediction - gamma*sdVec
plot(xGrid, upperBound, type="l", ylim = c(min(lowerBound), max(upperBound)),
     main = "Simulated Curves with Confidence Envelope", xlab = "Location", ylab = "Measured Value")
envelopePlot(xGrid, y1 = lowerBound, y2 = upperBound)
for(i in 1:10) {
  lines(uncertainty$x.grid, uncertainty$g.draw[,i], col="gray")
}

```

We can see that the simulations get farther apart, meaning the confidence envelope gets wider, where there aren't many data points and especially at the edges of the region.

## Two dimensional example

For another, more practical example, we will predict the average daily mean spring temperature for locations throughout Colorado. Using the data set \ttt{COmonthlyMet}, we can make a surface showing our predictions over a range of longitudes and latitudes, use the \ttt{US} function to draw in the USA state borders to show where Colorado is, and draw the points where data was recorded. Notice that \ttt{LatticeKrig} will automatically discard any data points with missing values (NAs) if needed.

```{r QuickStart2D}
data(COmonthlyMet)
locations <- CO.loc
observations <- CO.tmean.MAM.climate
kFitWeather <- LatticeKrig(locations, observations)
print(kFitWeather)
```



```{r QuickStart2DPlot1}
surface(kFitWeather, main = "Spring Temperature Estimates across Colorado",
        xlab="Longitude", ylab="Latitude")
points(locations, pch = '*')
US(add=TRUE, col='black', lwd=4)
```

This plot is useful, but we can do better. We can see that the coldest temperatures are in the Rocky Mountains at higher elevations, which is not surprising. Thus, we might expect that we will get a more accurate fit by having \ttt{LatticeKrig} account for the elevation at each location as well. Another way we can improve the plot is by increasing its resolution - the current image is somewhat pixelated. The \ttt{surface} function will evaluate the surface at more points if we increase the \ttt{nx} and \ttt{ny} arguments: setting \ttt{nx=200, ny=150} will produce a grid of 30,000 points, which will take longer to compute but produces a nicer looking, more detailed plot. Finally, we can also have \ttt{surface} extend the evaluation all the way to the corners of the window by using the \ttt{extrap} argument; by default it doesn't extrapolate outside of the existing data, since the error often increases dramatically when predicting outside of the given data. However, extending the plot to the corners will make it look nicer. For the sake of example, we will also change the color scale in the image by setting the \ttt{col} parameter.

```{r QuickStart2DImproved}
data(COmonthlyMet)
locations <- CO.loc
observations <- CO.tmean.MAM.climate
elevations <- CO.elev
kFitWeather <- LatticeKrig(locations, observations, Z=cbind(elevations))
print(kFitWeather)
```

Compared to the previous fit, we can see that this new fit with the covariate has much fewer effective degrees of freedom and a much lower \ttt{rho}, which means that the covariate was able to explain a significant amount of the variation in the data. 

```{r QuickStart2DPlot2}
# look at the help file in fields for information on the grid.list format
prediction <- predictSurface(kFitWeather, grid.list = CO.Grid, ZGrid = CO.elevGrid,
                             nx = 200, ny = 150, extrap = TRUE)
surface(prediction, main = "Improved Spring Temperature Estimates across Colorado", 
        xlab="Longitude", ylab="Latitude", col=larry.colors())
US(add=TRUE, col='black', lwd=4)
```

This surface is so rough because it accounts for elevation; we can see that the plot is fairly smooth in the eastern half of the state, and extremely rough in the mountains. 

Finally, it is important to note some potential issues that \ttt{LatticeKrig} calculations won't catch. Because \ttt{LatticeKrig} estimates some parameters of the data, the model could be a poor fit if the estimates aren't reasonable. See section 6.4 for more details on this. The \ttt{LatticeKrig} model also approximates a thin plate spline by default, which may not be a good fit for a given problem. Finally, as with other curve fitting techniques, you should examine the residuals of the model for any patterns or features that may indicate a poor fit.

## Simulating a spatial process from the LatticeKrig model
 
As a final topic we describe how to generate realizations from the Gaussian model in this package. The \ttt{LKinfo} object has a full description of the model and so simulation is easy. In the two examples of this section this object was set up from the top level function \ttt{LatticeKrig} and is the LKinfo component of the returned results. For more control over the model, however,  we recommend that this object be created separately. (See Section 3)

For 
\ttt{kFit1D} from Section 2.1 note that a listing of the full model is shown from.
```{r eval=FALSE}
print(kFit1D$LKinfo)
```

Here we simulate 4 sample curves from this model and evaluate them on a finer grid of points than the observations. The random seed is set to reproduce these particular psuedo random draws. 

```{r }
set.seed(123)
gSim <- LKrig.sim(xGrid, kFit1D$LKinfo, M=4)
matplot(xGrid, gSim, type="l", lty=1, xlab="x", ylab="g(x)")
title("Simulated curves from LK model")
```
Note that in actually fitting the data a linear function is also included but since this is not a random component it is not part of the simulated process. Also the variance of the process is set to one. 

These simulated curves are referred to as *unconditional* because they are
unrelated to the actual data except in terms of the range of the x values.
Another form of simulation is to generate the process *conditional* on
the observed data. This technique turns out to be very useful for quantifying the uncertianty in the curve estimate, and is the Monte Carlo method used to generate standard errors and confidence envelopes in the previous section. The example below creates 25 conditional draws from fitting the 1D example. All these curves are "equally likely" or plausible given the observations. This function returns several different parts of the estimate and so a list format is used. Note the use of the predict function to recover the estimated curve and also that the data is part of the fitted object. Within the range of the data all the conditional curves tend to track the estimate and the data, however, as one might expect beyond on the range of the observations there is much more variability among the simulated curves.   

```{r }
set.seed(123)
gCondSim <- LKrig.sim.conditional(kFit1D, M=25, x.grid=as.matrix(xGrid))
matplot(xGrid, gCondSim$g.draw, type="l", lty=1,xlab="x", ylab="y", col="grey",
        main="Estimating minimum with conditional simulation", lwd=.5)
lines(xGrid, predict(kFit1D, xGrid), col="red")
points(kFit1D$x, kFit1D$y, pch=16, col="black")
```

## Extra credit!

Here is a final example illustrating the power of determining the uncertainty by Monte Carlo. We generate a larger conditional sample over the subinterval [-3,0], find the minimum of each realization and plot the minimum and its location. Note that this represents uncertainty in both the minimum value itself and its location on the x-axis. Of course, because we know the true curve is a sine wave, we know the true minimum is exactly -1 at $x= -\pi / 2$. This two dimensional distribution of minima and their locations is a valid approach to approximate the uncertainty of the estimated minimum of the true curve in this range. It would be difficult to derive an analytic formula for this distribution so the Monte Carlo approach is quite useful. 

```{r }
xGrid2 <- as.matrix(seq(-3, 0, length.out=100))
set.seed(333)
suppressMessages(
  gCondSim <- LKrig.sim.conditional(kFit1D, x.grid=xGrid2, M=75)
)
# index of where minimum occurs in each draw
minXIndex <- apply(gCondSim$g.draw, 2, which.min)
XMin <- xGrid2[minXIndex]
YMin <- apply(gCondSim$g.draw, 2, min)
matplot(xGrid2, gCondSim$g.draw, type="l", lty=1,
        xlab="x", ylab="y", col="grey", ylim=c()) 
points(XMin, YMin, col="red4", cex=.5)         
lines(xGrid2, predict(kFit1D, xGrid2), col="red")
points(kFit1D$x,kFit1D$y, pch=16, col="black")        
```